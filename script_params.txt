-train_src data/multi30k/train2016.en.atok -train_tgt data/multi30k/train2016.de.atok -valid_src data/multi30k/val.en.atok -valid_tgt data/multi30k/val.de.atok -save_data data/multi30k.atok.low -lower
***Preprocess***
-train_src data/wmt14/train.en -train_tgt data/wmt14/train.de -valid_src data/wmt14/newstest2014.en -valid_tgt data/wmt14/newstest2014.de -save_data data/wmt14.atok.low -lower
>>Reversed
-train_src data/wmt14/train_reversed.en -train_tgt data/wmt14/train.de -valid_src data/wmt14/newstest2013_reversed.en -valid_tgt data/wmt14/newstest2013.de -save_data data/wmt14.atok.low_reversed_newstest2013_torch04 -lower


***Train***
-data data/wmt14.atok.low.train -save_model wmt14_model -gpuid 0 -encoder_type brnn -decoder_type rnn -dropout 0.2 -epochs 2 -global_attention general -input_feed 1 -report_every 1000
>>Reversed
-data data/wmt14.atok.low.train_reversed -save_model wmt14_model_reversed -gpuid 0 -encoder_type brnn -decoder_type rnn -dropout 0.2 -epochs 2 -global_attention general -input_feed 1 -report_every 1000

>>Baseline + global attention (general)
--Train
-data data/wmt14.atok.low.train_reversed -save_model wmt14_model_global-general -gpuid 0 -encoder_type brnn -decoder_type rnn -word_vec_size 1000 -rnn_size 1000 -layers 4 -batch_size 128 -dropout 0.2 -epochs 12 -global_attention general -input_feed 0 -report_every 1000 -tensorboard -tensorboard_log_dir runs/onmt

-data data/wmt14.atok.low_reversed -save_model wmt14_model_global-general -gpuid 0 -encoder_type brnn -decoder_type rnn -word_vec_size 1000 -rnn_size 1000 -layers 4 -batch_size 128 -dropout 0.2 -epochs 12 -global_attention general -input_feed 0 -report_every 1000 -tensorboard -tensorboard_log_dir runs/onmt -train_from wmt14_model_global-general_acc_60.66_ppl_6.11_e7.pt
>> Baseline + global attention (dot) + inputfeed
python train.py -data data/wmt14.atok.low_reversed_newstest2013_torch04 -save_model models/wmt14_model_local-dot_inputfeed -gpuid 0 -encoder_type brnn -decoder_type rnn -word_vec_size 1000 -rnn_size 1000 -layers 4 -batch_size 128 -dropout 0.2 -epochs 12 -global_attention dot -input_feed 1 -report_every 1000 -tensorboard -tensorboard_log_dir runs/local-attention-dot-inputfeed

python train.py -data data/wmt14.atok.low_reversed_newstest2013_torch04 -save_model models/wmt14_model_local-dot_inputfeed_test -encoder_type brnn -decoder_type rnn -word_vec_size 1000 -rnn_size 1000 -layers 4 -batch_size 128 -dropout 0.2 -epochs 12 -attention_model local-p -attention_score_function dot -input_feed 1 -report_every 1000 -tensorboard -tensorboard_log_dir runs/local-attention-dot-inputfeed_test

--Translate
python translate.py -model wmt14_model_global-general_acc_62.43_ppl_5.54_e12.pt -src data/wmt14/newstest2014_reversed.en -tgt data/wmt14/newstest2014.de -verbose -output wmt14.test.pred.atok.global-general_e12 -beam_size 10

